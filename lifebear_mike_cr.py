# -*- coding: utf-8 -*-
"""Lifebear_Mike_CR

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RIUJ_NMahdrDfQHAkv32DXf7nWaPxQIq
"""

import pandas as pd
import os
import re

dataset = f'/content/lifebear.csv'

lifebear = pd.read_csv(dataset, sep=';', low_memory=True)
lifebear.info()

# Find duplicate rows based on 'mail_address' and 'login_id'
duplicate_rows = lifebear[lifebear.duplicated(subset=['mail_address', 'login_id'], keep=False)]

# Drop duplicates, keeping the first occurrence
lifebear.drop_duplicates(subset=['mail_address', 'login_id'], keep='first', inplace=True)

# Save the duplicates and unique rows to separate CSV files
duplicate_rows.to_csv('lifebear_dump.csv', index=False)
lifebear.to_csv('lifebear_single.csv', index=False)

dump = pd.read_csv('lifebear_dump.csv')
single = pd.read_csv('lifebear_single.csv')

def split_csv(input_file, output_dir, chunk_size_mb=100):
  """Splits a CSV file into smaller chunks based on size.

  Args:
    input_file: Path to the input CSV file.
    output_dir: Directory to save the output chunks.
    chunk_size_mb: Size of each chunk in MB.
  """

  if not os.path.exists(output_dir):
    os.makedirs(output_dir)

  chunk_size_bytes = chunk_size_mb * 1024 * 1024
  current_chunk_size = 0
  current_chunk_num = 1
  current_chunk_df = pd.DataFrame()

  for chunk in pd.read_csv(input_file, chunksize=1000000):  # Adjust chunksize as needed
    current_chunk_df = pd.concat([current_chunk_df, chunk], ignore_index=True)
    current_chunk_size += current_chunk_df.memory_usage(deep=True).sum()

    if current_chunk_size >= chunk_size_bytes:
      output_file = os.path.join(output_dir, f"lifebear_chunk_{current_chunk_num}.csv")
      current_chunk_df.to_csv(output_file, index=False)
      current_chunk_size = 0
      current_chunk_num += 1
      current_chunk_df = pd.DataFrame()

  if not current_chunk_df.empty:
    output_file = os.path.join(output_dir, f"lifebear_chunk_{current_chunk_num}.csv")
    current_chunk_df.to_csv(output_file, index=False)


# Example usage:
input_file = '/content/lifebear_single.csv'
output_dir = '/content/chunks'
split_csv(input_file, output_dir)

def validate_email(email):
  """Validates an email address using a regular expression."""
  pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
  return bool(re.match(pattern, email))

# Directories for cleaned and invalid chunks
cleaned_dir = '/content/cleaned_chunks'
garbage_dir = '/content/garbage'

if not os.path.exists(cleaned_dir):
  os.makedirs(cleaned_dir)

if not os.path.exists(garbage_dir):
  os.makedirs(garbage_dir)


for filename in os.listdir('/content/chunks'):
  if filename.startswith('lifebear_chunk_') and filename.endswith('.csv'):
    filepath = os.path.join('/content/chunks', filename)
    df = pd.read_csv(filepath)

    # Validate email addresses
    valid_entries = df[df['mail_address'].apply(validate_email)]
    invalid_entries = df[~df['mail_address'].apply(validate_email)]

    # Save valid and invalid entries to separate files
    chunk_num = int(filename.split('_')[2].split('.')[0])
    valid_entries.to_csv(os.path.join(cleaned_dir, f'lifebear_clean_{chunk_num}.csv'), index=False)
    invalid_entries.to_csv(os.path.join(garbage_dir, f'lifebear_invalid_{chunk_num}.csv'), index=False)

print("Validation and saving completed.")

for filename in os.listdir('/content/cleaned_chunks'):
  if filename.startswith('lifebear_clean_') and filename.endswith('.csv'):
    filepath = os.path.join('/content/cleaned_chunks', filename)
    df = pd.read_csv(filepath)

    print(f"\nProcessing chunk: {filename}")
    print("Info:")
    print(df.info())
    print("\nHead:")
    print(df.head())
    print("\nTail:")
    print(df.tail())
    print("\nDescribe:")
    print(df.describe())

print("Looping and displaying completed.")

# Loop through chunks in '/content/garbage' and show info, head, tail, describe
for filename in os.listdir('/content/garbage'):
  if filename.startswith('lifebear_invalid_') and filename.endswith('.csv'):
    filepath = os.path.join('/content/garbage', filename)
    try:
      df = pd.read_csv(filepath)
      print(f"\nProcessing: {filename}")
      print("Info:")
      print(df.info())
      print("\nHead:")
      print(df.head())
      print("\nTail:")
      print(df.tail())
      print("\nDescribe:")
      print(df.describe())
    except Exception as e:
      print(f"Error processing {filename}: {e}")

# Combine all chunks in '/content/cleaned_chunks' and save to '/content/clean/lifebear_clean.csv'
combined_df = pd.DataFrame()
for filename in os.listdir('/content/cleaned_chunks'):
  if filename.startswith('lifebear_clean_') and filename.endswith('.csv'):
    filepath = os.path.join('/content/cleaned_chunks', filename)
    df = pd.read_csv(filepath)
    combined_df = pd.concat([combined_df, df], ignore_index=True)

os.makedirs('/content/clean', exist_ok=True)
combined_df.to_csv('/content/clean/lifebear_clean.csv', index=False)

print("Combined cleaned chunks saved to /content/clean/lifebear_clean.csv")

# Combine all chunks in '/content/garbage' and save to '/content/invalid/lifebear_invalid.csv'
combined_invalid_df = pd.DataFrame()
for filename in os.listdir('/content/garbage'):
  if filename.startswith('lifebear_invalid_') and filename.endswith('.csv'):
    filepath = os.path.join('/content/garbage', filename)
    try:
      df = pd.read_csv(filepath)
      combined_invalid_df = pd.concat([combined_invalid_df, df], ignore_index=True)
    except Exception as e:
      print(f"Error processing {filename}: {e}")

os.makedirs('/content/invalid', exist_ok=True)
combined_invalid_df.to_csv('/content/invalid/lifebear_invalid.csv', index=False)

print("Combined invalid chunks saved to /content/invalid/lifebear_invalid.csv")